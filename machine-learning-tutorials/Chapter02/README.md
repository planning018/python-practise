## 第二章 创建分类器
知识点

分类是指利用数据的特性将其分成若干类型的过程。分类与回归不同，回归的输出结果是实数。

### 建立逻辑回归分类器

逻辑回归其实是一种分类方法。给定一组数据点，需要建立一个可以在类之间绘制线性边界的模型。逻辑回归就可以对训练数据派生的一组方程进行求解来提取边界。

```python
from sklearn import linear_model
```

### 建立朴素贝叶斯分类器

朴素贝叶斯分类器是用贝叶斯定理进行建模的监督学习分类器。

```python
from sklearn.naive_bayes import GaussianNB
from logistic_regression import plot_classifier
```

机器学习的一条最佳实践是用没有重叠（nonoverlapping）的数据进行训练和测试。

### 用交叉验证检验模型准确性

交叉验证是机器学习的重要概率。

过度拟合是指模型在已知数据集上拟合的超级好，但是一遇到未知数据就挂了。但我们希望模型能够适用于未知数据。

当处理机器学习模型时，通常关心3个指标：精度（precision）、召回率（recall）和 F1得分。

* 精度是指被分类器正确分类的样本数量占分类器总分类样本数量的百分比（分类器分类结果中，有一些样本分错了）
* 召回率是指被应正确分类的样本数量占某分类总样本数量的百分比（有一些样本属于某分类，但分类器却没有分出来）
* 为了量化两个指标的均衡性，引入了 F1 得分指标，是精度和召回率的合成指标，即两者的调和均值。

### 生成验证曲线

分类器的性能是由 超参数 决定的，当改变超参数时，希望可以看到分类器性能的变化情况。

### 生成学习曲线

学习曲线可以帮助我们理解训练数据集的大小对机器学习模型的影响。当遇到计算能力限制时，这一点非常有用。

训练数据集的规模越小，彷佛训练准确性越高，但是容易导致过度拟合。如果选择较大规模的训练数据集，就会消耗更多的资源。





## 第三章 预测建模

### 用 SVM 建立线性分类器

SVM（Support Vector Machines）支持向量机

SVM 是用来构建分类器和回归器的监督学习模型。SVM 通过对数学方程组求解，可以找出两组数据之间的最佳分割边界。

### 用 SVM 建立非线性分类器

SVM 为建立非线性分类器提供了很多选项，需要用不同的核函数建立非线性分类器。为了简单起见，考虑两种情况，当想要表示两种类型数据的曲线边界时，既可以用多项式函数，也可以用 径向基函数（Radial Basis Function，RBF）。

### 解决类型数量不平衡问题

```python
params = {'kernel':'linear', 'class_weight':'auto'}
```

参数 class_weight 的作用是统计不同类型数据点的数量，调整权重，让类型不平衡问题不影响分类效果。

### 提取置信度

如果能够获取对未知数据进行分类的置信水平，那将会非常有用。当一个新的数据点被分类为某一个已知类别时，我们可以训练 SVM 来计算出输出类型的置信度。

概率输出是一种将不同类别的距离度量转换成概率度量的方法。

```python
params = {'kernel':'rbf', 'probability': True}
classifier = SVC(**params)
```

参数 probability 用于告诉 SVM 训练的时候要计算出概率。

### 寻找最佳超参数

交叉验证

两个实践案例



## 第四章 无监督学习- 聚类

无监督学习是一种对不含标记的数据建立模型的机器学习范式。

最常见的无监督学习方法就是 聚类。

欧式距离

无监督学习广泛应用于 数据挖掘、医学影像、股票市场分析、计算机视觉、市场细分等。

### 用 k-means 算法聚类数据

k-means 算法是最流行的聚类算法之一。

这个算法常常利用数据的不同属性将输入数据划分为 k 组。分组是使用最优化的技术实现的，即让各组的数据点与该组中心点的距离平方和最小化。

### 用矢量量化压缩图片

k-means 聚类的主要应用之一就是 矢量量化。简单来说，矢量量化就是 四舍五入（rounding off）的N维版本。

我们用比原始图像更少的比特数来存储每个像素，从而实现图像图片。

### 建立均值漂移聚类模型

均值漂移是一种非常强大的无监督学习算法，该算法把数据点的分布看成是 概率密度函数（probability-density-function），希望在特征空间中根据函数分布特征找出数据点的 模式（mode）。

这些模式就对应于一群群局部最密集（local maxima）分布的点。均值漂移算法的优点是它不需要事先确定集群的数量。

### 用凝聚层次聚类进行数据分组

层次聚类（hierarchical clutering）是一组聚类算法，通过不断地分解或合并集群来构建来构建树状集群（tree-like clusters），层次聚类的结构可以用一棵树表示。

层次聚类算法可以是自下而上的，也可以是自上而下的。在自下而上的算法中，每个数据点都被看作是一个单独的集群。这些集群不断地合并，直到所有的集群都合并成一个巨型集群，这被称为 **凝聚层次聚类**。

### 评价聚类算法的聚类效果

在监督学习中，可以用预测值与原始值进行比较来计算模型的准确性。

度量聚类算法的一个好方法是观察集群被分离的离散程度。这些集群是不是被分离得很合理？一个集群中所有的数据点是不是足够紧密？于是我们采用一个被称为 **轮廓系数**（Silhouette Coefficient）得分的指标。

### 用 DBSCAN 算法自动估算集群数量

为了可以直接找出集群数量，于是出现了 **DBSCAN**（Density-Based Spatial Clustering of Applications with Noise）算法。

DBSCAN 将数据点看成是紧密集群的若干组。如果某个点属于一个集群，那么就应该有许多点也属于同一个集群。如果有一些点位于数据稀疏区域，DBSCAN 就会把这些点作为异常点，而不会强制将它们放入一个集群中。

实践案例：建立客户细分模型













